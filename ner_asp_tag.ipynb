{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39caed68-75ea-48ee-b456-e5784ce85d7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T06:41:55.648862Z",
     "iopub.status.busy": "2024-12-30T06:41:55.648409Z",
     "iopub.status.idle": "2024-12-30T06:43:30.554014Z",
     "shell.execute_reply": "2024-12-30T06:43:30.553504Z",
     "shell.execute_reply.started": "2024-12-30T06:41:55.648840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.1.99)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.66.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=c8d7df28c1f896520fdb6d509a7c261469071dcf62c8439a919eaa2e55ecd911\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.16.1+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence_transformers) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2020.6.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->evaluate) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2023.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow==2.16.1\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (16.0.6)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.16.1)\n",
      "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.60.0)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow==2.16.1)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow==2.16.1)\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (0.35.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.16.1) (1.26.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.16.1) (0.35.1)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow==2.16.1)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow==2.16.1)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow==2.16.1)\n",
      "  Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow==2.16.1) (2020.6.20)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.1) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.1) (2.1.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow==2.16.1)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow==2.16.1) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (391 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.8/391.8 kB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, ml-dtypes, mdurl, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.2.0\n",
      "    Uninstalling ml-dtypes-0.2.0:\n",
      "      Successfully uninstalled ml-dtypes-0.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.15.1\n",
      "    Uninstalling tensorboard-2.15.1:\n",
      "      Successfully uninstalled tensorboard-2.15.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "Successfully installed keras-3.7.0 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 optree-0.13.1 rich-13.9.4 tensorboard-2.16.2 tensorflow-2.16.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.3)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.3)\n",
      "Requirement already satisfied: lxml in /usr/lib/python3/dist-packages (from sacrebleu) (4.8.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, sacrebleu\n",
      "Successfully installed portalocker-3.0.0 sacrebleu-2.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.11/dist-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (0.35.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow[and-cuda]) (1.26.3)\n",
      "Collecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.35.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2020.6.20)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (2.1.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow[and-cuda]) (0.1.2)\n",
      "Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "Successfully installed nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install rouge\n",
    "!pip install sentencepiece\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install sentence_transformers\n",
    "!pip install evaluate\n",
    "!pip install tensorflow==2.16.1\n",
    "!pip install sacrebleu\n",
    "!pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df392ccd-0f8e-4639-86fc-5facca68a30c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T06:43:30.555068Z",
     "iopub.status.busy": "2024-12-30T06:43:30.554904Z",
     "iopub.status.idle": "2024-12-30T06:43:37.423882Z",
     "shell.execute_reply": "2024-12-30T06:43:37.423422Z",
     "shell.execute_reply.started": "2024-12-30T06:43:30.555052Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 06:43:34.764721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-30 06:43:35.546215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModel  # Not used, can be removed\n",
    "import multiprocessing\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import evaluate\n",
    "import rouge_score\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6280a76b-2835-4ede-9cab-cdef300c48ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T11:17:36.329742Z",
     "iopub.status.busy": "2024-12-27T11:17:36.329220Z",
     "iopub.status.idle": "2024-12-27T11:17:36.375797Z",
     "shell.execute_reply": "2024-12-27T11:17:36.375232Z",
     "shell.execute_reply.started": "2024-12-27T11:17:36.329722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled_dataset:\n",
      "Original size: 369\n",
      "Sampled size: 18\n",
      "\n",
      "Serum:\n",
      "Original size: 1055\n",
      "Sampled size: 50\n",
      "\n",
      "Moisturizer:\n",
      "Original size: 1299\n",
      "Sampled size: 62\n",
      "\n",
      "Eyecream:\n",
      "Original size: 1181\n",
      "Sampled size: 56\n",
      "\n",
      "Sunscreen:\n",
      "Original size: 1681\n",
      "Sampled size: 80\n",
      "\n",
      "Toner:\n",
      "Original size: 867\n",
      "Sampled size: 41\n",
      "\n",
      "Bodywash:\n",
      "Original size: 1337\n",
      "Sampled size: 63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add these imports for stratified sampling from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Load all CSV files from the current directory\n",
    "csv_files = [f for f in os.listdir() if f.endswith('.csv')]\n",
    "dataframes = {}\n",
    "\n",
    "# Create dictionary of dataframes\n",
    "for file in csv_files:\n",
    "    category_name = file.replace('.csv', '')\n",
    "    dataframes[category_name] = pd.read_csv(file)\n",
    "\n",
    "# Calculate total population size\n",
    "total_population = sum(len(df) for df in dataframes.values())\n",
    "\n",
    "# Calculate sample sizes for each stratum (95% confidence level, 5% margin of error)\n",
    "def calculate_sample_size(population_size, confidence=0.95, margin_error=0.05):\n",
    "    z_score = 1.96  # for 95% confidence level\n",
    "    sample_size = (z_score**2 * 0.25 * population_size) / ((margin_error**2 * (population_size-1)) + (z_score**2 * 0.25))\n",
    "    return int(np.ceil(sample_size))\n",
    "\n",
    "# Create dictionary for sampled dataframes\n",
    "sampled_dataframes = {}\n",
    "\n",
    "# Perform stratified sampling for each category\n",
    "for category, df in dataframes.items():\n",
    "    stratum_size = len(df)\n",
    "    # Calculate proportional sample size for this stratum\n",
    "    proportion = stratum_size / total_population\n",
    "    stratum_sample_size = int(np.ceil(calculate_sample_size(total_population) * proportion))\n",
    "    \n",
    "    # Perform random sampling\n",
    "    sampled_dataframes[category] = df.sample(n=min(stratum_sample_size, stratum_size), random_state=42)\n",
    "    \n",
    "    print(f\"{category}:\")\n",
    "    print(f\"Original size: {stratum_size}\")\n",
    "    print(f\"Sampled size: {len(sampled_dataframes[category])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2667729-13ec-4f40-bd71-9f6ca7d48310",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T11:14:48.296381Z",
     "iopub.status.busy": "2024-12-27T11:14:48.296077Z",
     "iopub.status.idle": "2024-12-27T11:14:48.343338Z",
     "shell.execute_reply": "2024-12-27T11:14:48.342810Z",
     "shell.execute_reply.started": "2024-12-27T11:14:48.296352Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir() if f.endswith('.csv')]\n",
    "dataframes = {}\n",
    "\n",
    "# Create dictionary of dataframes\n",
    "for file in csv_files:\n",
    "    category_name = file.replace('.csv', '')\n",
    "    df = pd.read_csv(file)\n",
    "    # Rename the single column to 'review_text'\n",
    "    df.columns = ['review_text']\n",
    "    dataframes[category_name] = df    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "672e03f7-0ee9-481a-bd44-9a439d6a03ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T11:14:55.463403Z",
     "iopub.status.busy": "2024-12-27T11:14:55.462775Z",
     "iopub.status.idle": "2024-12-27T11:14:56.074844Z",
     "shell.execute_reply": "2024-12-27T11:14:56.074363Z",
     "shell.execute_reply.started": "2024-12-27T11:14:55.463384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Serum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best sample for Serum:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Serum: 100%|██████████| 10/10 [00:00<00:00, 218.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serum - Best Jensen-Shannon divergence: 0.0916\n",
      "Original size: 1055, Sample size: 53\n",
      "\n",
      "Processing Moisturizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best sample for Moisturizer:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Moisturizer: 100%|██████████| 10/10 [00:00<00:00, 227.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moisturizer - Best Jensen-Shannon divergence: 0.0881\n",
      "Original size: 1299, Sample size: 65\n",
      "\n",
      "Processing Eyecream...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best sample for Eyecream:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Eyecream: 100%|██████████| 10/10 [00:00<00:00, 221.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyecream - Best Jensen-Shannon divergence: 0.0828\n",
      "Original size: 1181, Sample size: 59\n",
      "\n",
      "Processing Sunscreen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding best sample for Sunscreen:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Sunscreen: 100%|██████████| 10/10 [00:00<00:00, 158.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunscreen - Best Jensen-Shannon divergence: 0.0675\n",
      "Original size: 1681, Sample size: 83\n",
      "\n",
      "Processing Toner...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best sample for Toner:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Toner: 100%|██████████| 10/10 [00:00<00:00, 271.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toner - Best Jensen-Shannon divergence: 0.1071\n",
      "Original size: 867, Sample size: 43\n",
      "\n",
      "Processing Bodywash...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best sample for Bodywash:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Finding best sample for Bodywash: 100%|██████████| 10/10 [00:00<00:00, 209.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bodywash - Best Jensen-Shannon divergence: 0.0929\n",
      "Original size: 1337, Sample size: 66\n",
      "\n",
      "Final Dataset Summary:\n",
      "Total samples: 369\n",
      "Serum: 53 samples\n",
      "Moisturizer: 65 samples\n",
      "Eyecream: 59 samples\n",
      "Sunscreen: 83 samples\n",
      "Toner: 43 samples\n",
      "Bodywash: 66 samples\n",
      "\n",
      "Token Statistics:\n",
      "Original unique tokens: 11874\n",
      "Sampled unique tokens: 3057\n",
      "Overall distribution similarity: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ... previous code remains the same ...\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Download required NLTK resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_token_distribution(texts):\n",
    "    \"\"\"Calculate token frequency distribution for a list of texts using NLTK\"\"\"\n",
    "    # Create tokenizer that removes punctuation\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # Process all texts\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):  # Check if text is a string\n",
    "            # Tokenize and convert to lowercase\n",
    "            tokens = tokenizer.tokenize(text.lower())\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "    return pd.Series(all_tokens).value_counts(normalize=True)\n",
    "\n",
    "def calculate_distribution_similarity(dist1, dist2, epsilon=1e-10):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence between two distributions\"\"\"\n",
    "    # Align distributions by filling missing values with 0\n",
    "    combined_index = dist1.index.union(dist2.index)\n",
    "    dist1_aligned = dist1.reindex(combined_index, fill_value=0)\n",
    "    dist2_aligned = dist2.reindex(combined_index, fill_value=0)\n",
    "    \n",
    "    # Add epsilon to avoid log(0) issues\n",
    "    dist1_aligned += epsilon\n",
    "    dist2_aligned += epsilon\n",
    "    m = 0.5 * (dist1_aligned + dist2_aligned)\n",
    "    \n",
    "    # Calculate Jensen-Shannon divergence\n",
    "    js_divergence = 0.5 * (\n",
    "        (dist1_aligned * np.log(dist1_aligned / m)).sum() +\n",
    "        (dist2_aligned * np.log(dist2_aligned / m)).sum()\n",
    "    )\n",
    "    return js_divergence\n",
    "\n",
    "\n",
    "# Number of sampling iterations to try\n",
    "N_ITERATIONS = 10\n",
    "best_samples = {}\n",
    "\n",
    "for category, df in dataframes.items():\n",
    "    print(f\"\\nProcessing {category}...\")\n",
    "    \n",
    "    # Get original token distribution\n",
    "    original_dist = get_token_distribution(df['review_text'])\n",
    "    \n",
    "    best_divergence = float('inf')\n",
    "    best_sample = None\n",
    "    \n",
    "    # Calculate sample size for this stratum\n",
    "    stratum_size = len(df)\n",
    "    proportion = stratum_size / total_population\n",
    "    stratum_sample_size = int(np.ceil(calculate_sample_size(total_population) * proportion))\n",
    "    \n",
    "    # Try multiple random samples\n",
    "    for i in tqdm(range(N_ITERATIONS), desc=f\"Finding best sample for {category}\"):\n",
    "        # Generate sample\n",
    "        current_sample = df.sample(n=min(stratum_sample_size, stratum_size), random_state=i)\n",
    "        sample_dist = get_token_distribution(current_sample['review_text'])\n",
    "        \n",
    "        # Calculate distribution similarity\n",
    "        divergence = calculate_distribution_similarity(original_dist, sample_dist)\n",
    "        \n",
    "        # Update best sample if current is better\n",
    "        if divergence < best_divergence:\n",
    "            best_divergence = divergence\n",
    "            best_sample = current_sample\n",
    "    \n",
    "    best_samples[category] = best_sample\n",
    "    print(f\"{category} - Best Jensen-Shannon divergence: {best_divergence:.4f}\")\n",
    "    print(f\"Original size: {len(df)}, Sample size: {len(best_sample)}\")\n",
    "\n",
    "# Combine all best samples into a final dataset\n",
    "final_dataset = pd.concat(best_samples.values(), ignore_index=True)\n",
    "\n",
    "# Save the final dataset\n",
    "final_dataset.to_csv('sampled_dataset.csv', index=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nFinal Dataset Summary:\")\n",
    "print(f\"Total samples: {len(final_dataset)}\")\n",
    "for category in best_samples:\n",
    "    print(f\"{category}: {len(best_samples[category])} samples\")\n",
    "\n",
    "# Optional: Print token statistics for verification\n",
    "print(\"\\nToken Statistics:\")\n",
    "original_tokens = get_token_distribution(pd.concat(dataframes.values())['review_text'])\n",
    "sampled_tokens = get_token_distribution(final_dataset['review_text'])\n",
    "print(f\"Original unique tokens: {len(original_tokens)}\")\n",
    "print(f\"Sampled unique tokens: {len(sampled_tokens)}\")\n",
    "print(f\"Overall distribution similarity: {calculate_distribution_similarity(original_tokens, sampled_tokens):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba5016c-f0cc-4b2e-993c-7aec17e34965",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:12:56.760915Z",
     "iopub.status.busy": "2024-12-30T03:12:56.760399Z",
     "iopub.status.idle": "2024-12-30T03:12:57.093274Z",
     "shell.execute_reply": "2024-12-30T03:12:57.092524Z",
     "shell.execute_reply.started": "2024-12-30T03:12:56.760889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199f9f97-c1c8-4e35-8feb-a30d414a8a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:19:59.872398Z",
     "iopub.status.busy": "2024-12-30T03:19:59.871733Z",
     "iopub.status.idle": "2024-12-30T03:20:00.037027Z",
     "shell.execute_reply": "2024-12-30T03:20:00.036527Z",
     "shell.execute_reply.started": "2024-12-30T03:19:59.872380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of annotated sentences: 1213\n",
      "\n",
      "Sample of the dataframe:\n",
      "                                            sentence  \\\n",
      "0                           use instead foundation .   \n",
      "1  like high spf feels really good put dries matt...   \n",
      "2          think face better condition since using .   \n",
      "3  downside shade range lightest still slightly d...   \n",
      "4                      probably gives little color .   \n",
      "\n",
      "                                              tokens  \\\n",
      "0                      [use, instead, foundation, .]   \n",
      "1  [like, high, spf, feels, really, good, put, dr...   \n",
      "2  [think, face, better, condition, since, using, .]   \n",
      "3  [downside, shade, range, lightest, still, slig...   \n",
      "4                [probably, gives, little, color, .]   \n",
      "\n",
      "                                              labels  \n",
      "0                                    [O, O, B_PT, O]  \n",
      "1  [O, B-EF-POS, I-EF-POS, O, O, O, O, O, B-AP-PO...  \n",
      "2                           [O, B_BP, O, O, O, O, O]  \n",
      "3  [O, B-VP-NEG, I-VP-NEG, I-VP-NEG, O, B-AP-NEG,...  \n",
      "4                             [O, O, O, B-AP-POS, O]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "def process_annotation_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for annotation in data['annotations']:\n",
    "        text = annotation[0]\n",
    "        entities = annotation[1]['entities']\n",
    "        \n",
    "        # Split text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        current_char_offset = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            # Tokenize the sentence\n",
    "            tokens = word_tokenize(sentence)\n",
    "            \n",
    "            # Initialize all labels as 'O'\n",
    "            labels = ['O'] * len(tokens)\n",
    "            \n",
    "            # Create character to token index mapping for this sentence\n",
    "            char_to_token = {}\n",
    "            current_char = 0\n",
    "            for token_idx, token in enumerate(tokens):\n",
    "                for char_idx in range(current_char, current_char + len(token)):\n",
    "                    char_to_token[char_idx] = token_idx\n",
    "                current_char += len(token) + 1\n",
    "            \n",
    "            # Apply entity labels for this sentence\n",
    "            has_tags = False\n",
    "            for start, end, label in entities:\n",
    "                # Adjust start and end positions relative to sentence\n",
    "                rel_start = start - current_char_offset\n",
    "                rel_end = end - current_char_offset\n",
    "                \n",
    "                # Skip if entity is not in this sentence\n",
    "                if rel_start < 0 or rel_end > len(sentence):\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    token_start = char_to_token[rel_start]\n",
    "                    token_end = char_to_token[rel_end-1]\n",
    "                    \n",
    "                    # Apply labels to tokens\n",
    "                    for i in range(token_start, token_end + 1):\n",
    "                        labels[i] = label\n",
    "                        has_tags = True\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            \n",
    "            # Only add sentences that have tags other than 'O'\n",
    "            if has_tags:\n",
    "                processed_data.append({\n",
    "                    'sentence': sentence,\n",
    "                    'tokens': tokens,\n",
    "                    'labels': labels\n",
    "                })\n",
    "            \n",
    "            current_char_offset += len(sentence) + 1  # +1 for the period\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Process all annotation files\n",
    "dfs = []\n",
    "for i in range(1, 6):\n",
    "    file_path = f'Annotation{i}.json'\n",
    "    df = process_annotation_file(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all dataframes\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Total number of annotated sentences: {len(final_df)}\")\n",
    "print(\"\\nSample of the dataframe:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8ca0e1-ff84-49ec-804f-57d05a91af8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:27:15.535916Z",
     "iopub.status.busy": "2024-12-30T03:27:15.535692Z",
     "iopub.status.idle": "2024-12-30T03:27:15.557705Z",
     "shell.execute_reply": "2024-12-30T03:27:15.557189Z",
     "shell.execute_reply.started": "2024-12-30T03:27:15.535899Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.head()\n",
    "final_df.to_csv('SentenceAnnotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d56b64c-fe80-47a0-8d0b-2dd235eff68a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:30:15.858452Z",
     "iopub.status.busy": "2024-12-30T03:30:15.858223Z",
     "iopub.status.idle": "2024-12-30T03:30:15.869010Z",
     "shell.execute_reply": "2024-12-30T03:30:15.868500Z",
     "shell.execute_reply.started": "2024-12-30T03:30:15.858436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TOTAL TAGS: 16150\n",
      "==================================================\n",
      "\n",
      "CLASS DISTRIBUTION:\n",
      "--------------------------------------------------\n",
      "O               Count: 11449.0  Percentage: 70.89%\n",
      "B_PT            Count: 714.0    Percentage: 4.42%\n",
      "B_BP            Count: 557.0    Percentage: 3.45%\n",
      "I_PT            Count: 400.0    Percentage: 2.48%\n",
      "B_IN            Count: 308.0    Percentage: 1.91%\n",
      "I_IN            Count: 200.0    Percentage: 1.24%\n",
      "I_BP            Count: 196.0    Percentage: 1.21%\n",
      "B_BR            Count: 160.0    Percentage: 0.99%\n",
      "B-VP-POS        Count: 155.0    Percentage: 0.96%\n",
      "B-TX-POS        Count: 124.0    Percentage: 0.77%\n",
      "I_BR            Count: 89.0     Percentage: 0.55%\n",
      "I-SC-POS        Count: 84.0     Percentage: 0.52%\n",
      "B-PK-POS        Count: 82.0     Percentage: 0.51%\n",
      "B-SC-POS        Count: 76.0     Percentage: 0.47%\n",
      "I-PK-POS        Count: 72.0     Percentage: 0.45%\n",
      "B-SE-POS        Count: 70.0     Percentage: 0.43%\n",
      "B-EF-POS        Count: 68.0     Percentage: 0.42%\n",
      "I-VP-POS        Count: 66.0     Percentage: 0.41%\n",
      "B-HY-POS        Count: 61.0     Percentage: 0.38%\n",
      "B-AP-POS        Count: 59.0     Percentage: 0.37%\n",
      "B-VP-NEU        Count: 55.0     Percentage: 0.34%\n",
      "I-TX-POS        Count: 55.0     Percentage: 0.34%\n",
      "B-VP-NEG        Count: 54.0     Percentage: 0.33%\n",
      "I-SE-POS        Count: 51.0     Percentage: 0.32%\n",
      "B-PK-NEU        Count: 50.0     Percentage: 0.31%\n",
      "B-AB-POS        Count: 47.0     Percentage: 0.29%\n",
      "B-PE-POS        Count: 41.0     Percentage: 0.25%\n",
      "B-HY-NEG        Count: 41.0     Percentage: 0.25%\n",
      "B-SE-NEG        Count: 39.0     Percentage: 0.24%\n",
      "I-SC-NEG        Count: 39.0     Percentage: 0.24%\n",
      "B-QU-POS        Count: 38.0     Percentage: 0.24%\n",
      "B-PE-NEG        Count: 34.0     Percentage: 0.21%\n",
      "I-EF-POS        Count: 34.0     Percentage: 0.21%\n",
      "B-SE-NEU        Count: 32.0     Percentage: 0.2%\n",
      "B-SC-NEG        Count: 31.0     Percentage: 0.19%\n",
      "I-QU-POS        Count: 30.0     Percentage: 0.19%\n",
      "B-PK-NEG        Count: 30.0     Percentage: 0.19%\n",
      "B-TX-NEG        Count: 29.0     Percentage: 0.18%\n",
      "I-VP-NEG        Count: 29.0     Percentage: 0.18%\n",
      "B-SC-NEU        Count: 28.0     Percentage: 0.17%\n",
      "I-AB-POS        Count: 24.0     Percentage: 0.15%\n",
      "I-AP-POS        Count: 21.0     Percentage: 0.13%\n",
      "B-DU-POS        Count: 21.0     Percentage: 0.13%\n",
      "I-VP-NEU        Count: 21.0     Percentage: 0.13%\n",
      "B_SW            Count: 20.0     Percentage: 0.12%\n",
      "I-PK-NEG        Count: 19.0     Percentage: 0.12%\n",
      "I-HY-POS        Count: 17.0     Percentage: 0.11%\n",
      "B-EF-NEG        Count: 16.0     Percentage: 0.1%\n",
      "I-PE-POS        Count: 14.0     Percentage: 0.09%\n",
      "B-TX-NEU        Count: 14.0     Percentage: 0.09%\n",
      "I-HY-NEG        Count: 14.0     Percentage: 0.09%\n",
      "I-PE-NEG        Count: 13.0     Percentage: 0.08%\n",
      "I-EF-NEG        Count: 13.0     Percentage: 0.08%\n",
      "B-EF-NEU        Count: 13.0     Percentage: 0.08%\n",
      "I-SC-NEU        Count: 12.0     Percentage: 0.07%\n",
      "I-SE-NEG        Count: 12.0     Percentage: 0.07%\n",
      "I-DU-POS        Count: 11.0     Percentage: 0.07%\n",
      "I-TX-NEG        Count: 10.0     Percentage: 0.06%\n",
      "B-AP-NEU        Count: 10.0     Percentage: 0.06%\n",
      "B-QU-NEU        Count: 9.0      Percentage: 0.06%\n",
      "B-PE-NEU        Count: 7.0      Percentage: 0.04%\n",
      "B-HY-NEU        Count: 6.0      Percentage: 0.04%\n",
      "B-AP-NEG        Count: 6.0      Percentage: 0.04%\n",
      "I-TX-NEU        Count: 6.0      Percentage: 0.04%\n",
      "I-AB-NEG        Count: 5.0      Percentage: 0.03%\n",
      "B-AB-NEU        Count: 4.0      Percentage: 0.02%\n",
      "I-AP-NEU        Count: 4.0      Percentage: 0.02%\n",
      "B-AB-NEG        Count: 4.0      Percentage: 0.02%\n",
      "B-DU-NEG        Count: 4.0      Percentage: 0.02%\n",
      "I_SW            Count: 4.0      Percentage: 0.02%\n",
      "I-AB-NEU        Count: 3.0      Percentage: 0.02%\n",
      "I-AP-NEG        Count: 3.0      Percentage: 0.02%\n",
      "B-QU-NEG        Count: 3.0      Percentage: 0.02%\n",
      "I-DU-NEU        Count: 3.0      Percentage: 0.02%\n",
      "I-DU-NEG        Count: 2.0      Percentage: 0.01%\n",
      "B-DU-NEU        Count: 2.0      Percentage: 0.01%\n",
      "I-EF-NEU        Count: 1.0      Percentage: 0.01%\n",
      "I-SE-NEU        Count: 1.0      Percentage: 0.01%\n",
      "I-QU-NEG        Count: 1.0      Percentage: 0.01%\n",
      "\n",
      "==================================================\n",
      "Most common tag: O (Count: 11449)\n",
      "Least common tag: I-QU-NEG (Count: 1)\n",
      "Imbalance Ratio (majority:minority): 11449.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Set display options to show all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def analyze_class_distribution(sentences):\n",
    "    # Flatten all tags into a single list\n",
    "    all_tags = []\n",
    "    for sentence_tags in sentences:\n",
    "        all_tags.extend(sentence_tags)\n",
    "    \n",
    "    # Count occurrences\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Convert to DataFrame for better visualization\n",
    "    df = pd.DataFrame.from_dict(tag_counts, orient='index', columns=['count'])\n",
    "    df = df.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total = df['count'].sum()\n",
    "    df['percentage'] = (df['count'] / total * 100).round(2)\n",
    "    \n",
    "    # Print statistics with clear separation\n",
    "    print(\"=\"*50)\n",
    "    print(\"TOTAL TAGS:\", total)\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\nCLASS DISTRIBUTION:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # Print each class with clear formatting\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"{idx:<15} Count: {row['count']:<8} Percentage: {row['percentage']}%\")\n",
    "    \n",
    "    # Calculate imbalance metrics\n",
    "    majority_class_size = df['count'].max()\n",
    "    minority_class_size = df['count'][df['count'] > 0].min()  # Only consider classes that appear\n",
    "    imbalance_ratio = majority_class_size / minority_class_size\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Most common tag: {df.index[0]} (Count: {majority_class_size})\")\n",
    "    print(f\"Least common tag: {df.index[-1]} (Count: {minority_class_size})\")\n",
    "    print(f\"Imbalance Ratio (majority:minority): {imbalance_ratio:.2f}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Use the function\n",
    "sentences = final_df['labels'].tolist()\n",
    "analyze_class_distribution(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12d3a9bd-abaf-4531-ae37-dd8c49c27b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:49:32.673290Z",
     "iopub.status.busy": "2024-12-30T03:49:32.672734Z",
     "iopub.status.idle": "2024-12-30T03:49:32.681143Z",
     "shell.execute_reply": "2024-12-30T03:49:32.680623Z",
     "shell.execute_reply.started": "2024-12-30T03:49:32.673269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags with count < 50:\n",
      "B-AB-NEG: 4 occurrences\n",
      "B-AB-NEU: 4 occurrences\n",
      "B-AB-POS: 47 occurrences\n",
      "B-AP-NEG: 6 occurrences\n",
      "B-AP-NEU: 10 occurrences\n",
      "B-DU-NEG: 4 occurrences\n",
      "B-DU-NEU: 2 occurrences\n",
      "B-DU-POS: 21 occurrences\n",
      "B-EF-NEG: 16 occurrences\n",
      "B-EF-NEU: 13 occurrences\n",
      "B-HY-NEG: 41 occurrences\n",
      "B-HY-NEU: 6 occurrences\n",
      "B-PE-NEG: 34 occurrences\n",
      "B-PE-NEU: 7 occurrences\n",
      "B-PE-POS: 41 occurrences\n",
      "B-PK-NEG: 30 occurrences\n",
      "B-QU-NEG: 3 occurrences\n",
      "B-QU-NEU: 9 occurrences\n",
      "B-QU-POS: 38 occurrences\n",
      "B-SC-NEG: 31 occurrences\n",
      "B-SC-NEU: 28 occurrences\n",
      "B-SE-NEG: 39 occurrences\n",
      "B-SE-NEU: 32 occurrences\n",
      "B-TX-NEG: 29 occurrences\n",
      "B-TX-NEU: 14 occurrences\n",
      "B_SW: 20 occurrences\n",
      "I-AB-NEG: 5 occurrences\n",
      "I-AB-NEU: 3 occurrences\n",
      "I-AB-POS: 24 occurrences\n",
      "I-AP-NEG: 3 occurrences\n",
      "I-AP-NEU: 4 occurrences\n",
      "I-AP-POS: 21 occurrences\n",
      "I-DU-NEG: 2 occurrences\n",
      "I-DU-NEU: 3 occurrences\n",
      "I-DU-POS: 11 occurrences\n",
      "I-EF-NEG: 13 occurrences\n",
      "I-EF-NEU: 1 occurrences\n",
      "I-EF-POS: 34 occurrences\n",
      "I-HY-NEG: 14 occurrences\n",
      "I-HY-POS: 17 occurrences\n",
      "I-PE-NEG: 13 occurrences\n",
      "I-PE-POS: 14 occurrences\n",
      "I-PK-NEG: 19 occurrences\n",
      "I-QU-NEG: 1 occurrences\n",
      "I-QU-POS: 30 occurrences\n",
      "I-SC-NEG: 39 occurrences\n",
      "I-SC-NEU: 12 occurrences\n",
      "I-SE-NEG: 12 occurrences\n",
      "I-SE-NEU: 1 occurrences\n",
      "I-TX-NEG: 10 occurrences\n",
      "I-TX-NEU: 6 occurrences\n",
      "I-VP-NEG: 29 occurrences\n",
      "I-VP-NEU: 21 occurrences\n",
      "I_SW: 4 occurrences\n",
      "\n",
      "Dataset Statistics:\n",
      "Total sentences: 1213\n",
      "Sentences with low frequency tags: 434\n"
     ]
    }
   ],
   "source": [
    "def extract_low_frequency_sentences(df, min_count=50):\n",
    "    # First get the tag distribution\n",
    "    all_tags = []\n",
    "    for tags in df['labels']:\n",
    "        all_tags.extend(tags)\n",
    "    tag_counts = Counter(all_tags)\n",
    "    \n",
    "    # Find tags that appear less than min_count times\n",
    "    low_frequency_tags = {tag for tag, count in tag_counts.items() if count < min_count}\n",
    "    \n",
    "    # Print tags with low frequency\n",
    "    print(f\"Tags with count < {min_count}:\")\n",
    "    for tag in sorted(low_frequency_tags):\n",
    "        print(f\"{tag}: {tag_counts[tag]} occurrences\")\n",
    "    \n",
    "    # Function to check if a sentence contains any low frequency tags\n",
    "    def contains_low_frequency_tags(tags):\n",
    "        return any(tag in low_frequency_tags for tag in tags)\n",
    "    \n",
    "    # Create new DataFrame with sentences containing low frequency tags\n",
    "    low_frequency_df = df[df['labels'].apply(contains_low_frequency_tags)].copy()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total sentences: {len(df)}\")\n",
    "    print(f\"Sentences with low frequency tags: {len(low_frequency_df)}\")\n",
    "    \n",
    "    return low_frequency_df\n",
    "\n",
    "# Extract sentences with low frequency tags\n",
    "low_frequency_df = extract_low_frequency_sentences(final_df, min_count=50)\n",
    "\n",
    "# Save the low frequency dataset if needed\n",
    "#low_frequency_df.to_csv('low_frequency_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ff8d761-4a96-474b-8df7-c4f277162fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T03:49:54.647577Z",
     "iopub.status.busy": "2024-12-30T03:49:54.646853Z",
     "iopub.status.idle": "2024-12-30T03:49:54.656842Z",
     "shell.execute_reply": "2024-12-30T03:49:54.656290Z",
     "shell.execute_reply.started": "2024-12-30T03:49:54.647516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like high spf feels really good put dries matt...</td>\n",
       "      <td>[like, high, spf, feels, really, good, put, dr...</td>\n",
       "      <td>[O, B-EF-POS, I-EF-POS, O, O, O, O, O, B-AP-PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>downside shade range lightest still slightly d...</td>\n",
       "      <td>[downside, shade, range, lightest, still, slig...</td>\n",
       "      <td>[O, B-VP-NEG, I-VP-NEG, I-VP-NEG, O, B-AP-NEG,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>also keeps getting greasy throughout day cool ...</td>\n",
       "      <td>[also, keeps, getting, greasy, throughout, day...</td>\n",
       "      <td>[O, O, O, B-TX-NEU, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>use tinted sunscreens combat white cast zinc w...</td>\n",
       "      <td>[use, tinted, sunscreens, combat, white, cast,...</td>\n",
       "      <td>[O, B_BP, I_BP, O, B-AP-NEG, I-AP-NEG, B_IN, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>certainly dewy finish dont like consider using...</td>\n",
       "      <td>[certainly, dewy, finish, dont, like, consider...</td>\n",
       "      <td>[O, B-AP-NEU, O, O, O, O, O, B_PT, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "1  like high spf feels really good put dries matt...   \n",
       "3  downside shade range lightest still slightly d...   \n",
       "5  also keeps getting greasy throughout day cool ...   \n",
       "7  use tinted sunscreens combat white cast zinc w...   \n",
       "9  certainly dewy finish dont like consider using...   \n",
       "\n",
       "                                              tokens  \\\n",
       "1  [like, high, spf, feels, really, good, put, dr...   \n",
       "3  [downside, shade, range, lightest, still, slig...   \n",
       "5  [also, keeps, getting, greasy, throughout, day...   \n",
       "7  [use, tinted, sunscreens, combat, white, cast,...   \n",
       "9  [certainly, dewy, finish, dont, like, consider...   \n",
       "\n",
       "                                              labels  \n",
       "1  [O, B-EF-POS, I-EF-POS, O, O, O, O, O, B-AP-PO...  \n",
       "3  [O, B-VP-NEG, I-VP-NEG, I-VP-NEG, O, B-AP-NEG,...  \n",
       "5           [O, O, O, B-TX-NEU, O, O, O, O, O, O, O]  \n",
       "7  [O, B_BP, I_BP, O, B-AP-NEG, I-AP-NEG, B_IN, O...  \n",
       "9  [O, B-AP-NEU, O, O, O, O, O, B_PT, O, O, O, O,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_frequency_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39087857-9020-4ec6-9390-03698c2e6c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T06:43:37.426682Z",
     "iopub.status.busy": "2024-12-30T06:43:37.426504Z",
     "iopub.status.idle": "2024-12-30T06:43:37.530692Z",
     "shell.execute_reply": "2024-12-30T06:43:37.530230Z",
     "shell.execute_reply.started": "2024-12-30T06:43:37.426667Z"
    }
   },
   "outputs": [],
   "source": [
    "final_complete_df = pd.read_csv('AugmentedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64f0d2b6-dd10-4570-9652-c82c697c7dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:07:37.451121Z",
     "iopub.status.busy": "2024-12-30T07:07:37.450612Z",
     "iopub.status.idle": "2024-12-30T07:07:38.817615Z",
     "shell.execute_reply": "2024-12-30T07:07:38.817165Z",
     "shell.execute_reply.started": "2024-12-30T07:07:37.451101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of first row:\n",
      "Tokens: ['use', 'instead', 'foundation', '.']\n",
      "Labels: ['O', 'O', 'B_PT', 'O']\n",
      "\n",
      "Preparing dataset...\n",
      "Number of unique labels: 79\n",
      "\n",
      "Label mappings:\n",
      "B-AB-NEG: 0\n",
      "B-AB-NEU: 1\n",
      "B-AB-POS: 2\n",
      "B-AP-NEG: 3\n",
      "B-AP-NEU: 4\n",
      "B-AP-POS: 5\n",
      "B-DU-NEG: 6\n",
      "B-DU-NEU: 7\n",
      "B-DU-POS: 8\n",
      "B-EF-NEG: 9\n",
      "B-EF-NEU: 10\n",
      "B-EF-POS: 11\n",
      "B-HY-NEG: 12\n",
      "B-HY-NEU: 13\n",
      "B-HY-POS: 14\n",
      "B-PE-NEG: 15\n",
      "B-PE-NEU: 16\n",
      "B-PE-POS: 17\n",
      "B-PK-NEG: 18\n",
      "B-PK-NEU: 19\n",
      "B-PK-POS: 20\n",
      "B-QU-NEG: 21\n",
      "B-QU-NEU: 22\n",
      "B-QU-POS: 23\n",
      "B-SC-NEG: 24\n",
      "B-SC-NEU: 25\n",
      "B-SC-POS: 26\n",
      "B-SE-NEG: 27\n",
      "B-SE-NEU: 28\n",
      "B-SE-POS: 29\n",
      "B-TX-NEG: 30\n",
      "B-TX-NEU: 31\n",
      "B-TX-POS: 32\n",
      "B-VP-NEG: 33\n",
      "B-VP-NEU: 34\n",
      "B-VP-POS: 35\n",
      "B_BP: 36\n",
      "B_BR: 37\n",
      "B_IN: 38\n",
      "B_PT: 39\n",
      "B_SW: 40\n",
      "I-AB-NEG: 41\n",
      "I-AB-NEU: 42\n",
      "I-AB-POS: 43\n",
      "I-AP-NEG: 44\n",
      "I-AP-NEU: 45\n",
      "I-AP-POS: 46\n",
      "I-DU-NEG: 47\n",
      "I-DU-NEU: 48\n",
      "I-DU-POS: 49\n",
      "I-EF-NEG: 50\n",
      "I-EF-NEU: 51\n",
      "I-EF-POS: 52\n",
      "I-HY-NEG: 53\n",
      "I-HY-POS: 54\n",
      "I-PE-NEG: 55\n",
      "I-PE-POS: 56\n",
      "I-PK-NEG: 57\n",
      "I-PK-POS: 58\n",
      "I-QU-NEG: 59\n",
      "I-QU-POS: 60\n",
      "I-SC-NEG: 61\n",
      "I-SC-NEU: 62\n",
      "I-SC-POS: 63\n",
      "I-SE-NEG: 64\n",
      "I-SE-NEU: 65\n",
      "I-SE-POS: 66\n",
      "I-TX-NEG: 67\n",
      "I-TX-NEU: 68\n",
      "I-TX-POS: 69\n",
      "I-VP-NEG: 70\n",
      "I-VP-NEU: 71\n",
      "I-VP-POS: 72\n",
      "I_BP: 73\n",
      "I_BR: 74\n",
      "I_IN: 75\n",
      "I_PT: 76\n",
      "I_SW: 77\n",
      "O: 78\n",
      "\n",
      "Verifying processed data:\n",
      "\n",
      "Example 1:\n",
      "Tokens: ['use', 'instead', 'foundation', '.']\n",
      "Labels: ['O', 'O', 'B_PT', 'O']\n",
      "\n",
      "Example 2:\n",
      "Tokens: ['like', 'high', 'spf', 'feels', 'really', 'good', 'put', 'dries', 'matte', 'finish', 'smooth', 'like', 'skin', '.']\n",
      "Labels: ['O', 'B-EF-POS', 'I-EF-POS', 'O', 'O', 'O', 'O', 'O', 'B-AP-POS', 'I-AP-POS', 'B-TX-POS', 'O', 'B_BP', 'O']\n",
      "\n",
      "Example 3:\n",
      "Tokens: ['think', 'face', 'better', 'condition', 'since', 'using', '.']\n",
      "Labels: ['O', 'B_BP', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    \"\"\"\n",
    "    Prepare dataset for training by:\n",
    "    1. Creating label mappings from the actual labels\n",
    "    2. Tokenizing and aligning labels\n",
    "    \"\"\"\n",
    "    # First, ensure labels are in the correct format (lists, not strings)\n",
    "    df['labels'] = df['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Create label mappings from existing labels\n",
    "    unique_labels = set()\n",
    "    for labels in df['labels']:\n",
    "        unique_labels.update(labels)\n",
    "    \n",
    "    # Sort labels to ensure consistent mapping\n",
    "    unique_labels = sorted(list(unique_labels))\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    \n",
    "    print(f\"Number of unique labels: {len(label2id)}\")\n",
    "    print(\"\\nLabel mappings:\")\n",
    "    for label, idx in label2id.items():\n",
    "        print(f\"{label}: {idx}\")\n",
    "    \n",
    "    return df, label2id, id2label\n",
    "\n",
    "# Let's verify the data format first\n",
    "print(\"Sample of first row:\")\n",
    "print(\"Tokens:\", final_complete_df['tokens'].iloc[0])\n",
    "print(\"Labels:\", final_complete_df['labels'].iloc[0])\n",
    "\n",
    "# Prepare the dataset\n",
    "print(\"\\nPreparing dataset...\")\n",
    "processed_df, label2id, id2label = prepare_dataset(final_complete_df)\n",
    "\n",
    "# Print first few examples to verify\n",
    "print(\"\\nVerifying processed data:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Tokens:\", processed_df['tokens'].iloc[i])\n",
    "    print(\"Labels:\", processed_df['labels'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28909a06-f4a9-48cc-ac41-5412e52b237a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:14:38.855869Z",
     "iopub.status.busy": "2024-12-30T07:14:38.855220Z",
     "iopub.status.idle": "2024-12-30T07:14:40.179235Z",
     "shell.execute_reply": "2024-12-30T07:14:40.178768Z",
     "shell.execute_reply.started": "2024-12-30T07:14:38.855844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying processed datasets:\n",
      "Training samples: 10462\n",
      "Validation samples: 2242\n",
      "Test samples: 2242\n",
      "\n",
      "Sample verification:\n",
      "Input shape: torch.Size([212])\n",
      "Label shape: torch.Size([212])\n",
      "\n",
      "Token-Label Alignment:\n",
      "▁love           -> B-PK-POS\n",
      "▁product        -> B_PT\n",
      "▁look           -> O\n",
      "▁working        -> O\n",
      "▁.              -> O\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    \"\"\"\n",
    "    Tokenize the text and align the labels with the tokens\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None  # Return list instead of tensors\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens get -100\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Subwords get -100\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Split the dataset\n",
    "train_df, temp_df = train_test_split(processed_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to features\n",
    "def convert_to_features(df, tokenizer, label2id):\n",
    "    return tokenize_and_align_labels(\n",
    "        {\n",
    "            'tokens': df['tokens'].tolist(),\n",
    "            'labels': df['labels'].tolist()\n",
    "        },\n",
    "        tokenizer,\n",
    "        label2id\n",
    "    )\n",
    "\n",
    "# Create features for each split\n",
    "train_features = convert_to_features(train_df, tokenizer, label2id)\n",
    "val_features = convert_to_features(val_df, tokenizer, label2id)\n",
    "test_features = convert_to_features(test_df, tokenizer, label2id)\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class ABSADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.features.items()}\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = ABSADataset(train_features)\n",
    "val_dataset = ABSADataset(val_features)\n",
    "test_dataset = ABSADataset(test_features)\n",
    "\n",
    "# Let's verify the processed datasets\n",
    "print(\"\\nVerifying processed datasets:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Verify a sample\n",
    "sample_idx = 0\n",
    "sample = train_dataset[sample_idx]\n",
    "print(\"\\nSample verification:\")\n",
    "print(\"Input shape:\", sample['input_ids'].shape)\n",
    "print(\"Label shape:\", sample['labels'].shape)\n",
    "\n",
    "# Decode a sample to verify alignment\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
    "labels = sample['labels']\n",
    "\n",
    "print(\"\\nToken-Label Alignment:\")\n",
    "for token, label_id in zip(tokens, labels):\n",
    "    if label_id != -100:  # Only show non-special tokens\n",
    "        label = id2label[label_id.item()]\n",
    "        print(f\"{token:15} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd45da2e-71df-47a7-9eed-5a9deac293bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:19:01.000213Z",
     "iopub.status.busy": "2024-12-30T07:19:00.999639Z",
     "iopub.status.idle": "2024-12-30T07:19:14.003768Z",
     "shell.execute_reply": "2024-12-30T07:19:14.003246Z",
     "shell.execute_reply.started": "2024-12-30T07:19:01.000213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING DATASET SAMPLES:\n",
      "\n",
      "Verifying 3 random samples:\n",
      "\n",
      "Sample 1493:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 5\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁enter               O                    1916       78        \n",
      "▁skin                B_BP                 1158       36        \n",
      "▁smell               O                    4984       78        \n",
      "▁moisturized         B-HY-NEG             65034      12        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 6: [SEP] -> Label ID: -100\n",
      "Position 7: [PAD] -> Label ID: -100\n",
      "Position 8: [PAD] -> Label ID: -100\n",
      "Position 9: [PAD] -> Label ID: -100\n",
      "Position 10: [PAD] -> Label ID: -100\n",
      "Position 11: [PAD] -> Label ID: -100\n",
      "Position 12: [PAD] -> Label ID: -100\n",
      "Position 13: [PAD] -> Label ID: -100\n",
      "Position 14: [PAD] -> Label ID: -100\n",
      "Position 15: [PAD] -> Label ID: -100\n",
      "Position 16: [PAD] -> Label ID: -100\n",
      "Position 17: [PAD] -> Label ID: -100\n",
      "Position 18: [PAD] -> Label ID: -100\n",
      "Position 19: [PAD] -> Label ID: -100\n",
      "Position 20: [PAD] -> Label ID: -100\n",
      "Position 21: [PAD] -> Label ID: -100\n",
      "Position 22: [PAD] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 4342:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 22\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁sometimes           O                    1359       78        \n",
      "▁use                 O                    380        78        \n",
      "▁cron                O                    55186      78        \n",
      "▁hazel               I_BR                 52233      74        \n",
      "▁toner               I_PT                 25118      76        \n",
      "▁sporting            O                    8275       78        \n",
      "▁residue             O                    16903      78        \n",
      "▁rub                 O                    11543      78        \n",
      "▁face                I_BP                 812        73        \n",
      "▁still               O                    449        78        \n",
      "▁feels               O                    2576       78        \n",
      "▁like                O                    334        78        \n",
      "▁residue             I-SE-NEG             16903      64        \n",
      "▁wait                O                    1495       78        \n",
      "▁p                   O                    845        78        \n",
      "▁pros                O                    9283       78        \n",
      "▁envision            O                    21369      78        \n",
      "▁overall             O                    1629       78        \n",
      "▁wallop              O                    89980      78        \n",
      "▁skin                I_BP                 1158       73        \n",
      "▁far                 O                    659        78        \n",
      "▁evil                O                    3876       78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 27: [SEP] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 562:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 15\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁lines               O                    1728       78        \n",
      "▁acne                O                    10280      78        \n",
      "▁youthful            B-VP-NEG             15321      33        \n",
      "▁look                I-VP-NEG             468        70        \n",
      "▁might               O                    520        78        \n",
      "▁try                 O                    687        78        \n",
      "▁disadvantageous     O                    112842     78        \n",
      "▁present             O                    910        78        \n",
      "▁mur                 B_BR                 42543      37        \n",
      "▁serious             O                    1721       78        \n",
      "▁hoo                 O                    38552      78        \n",
      "▁ever                O                    632        78        \n",
      "▁utilise             O                    25530      78        \n",
      "▁face                B_BP                 812        36        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 19: [SEP] -> Label ID: -100\n",
      "Position 20: [PAD] -> Label ID: -100\n",
      "Position 21: [PAD] -> Label ID: -100\n",
      "Position 22: [PAD] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "VALIDATION DATASET SAMPLES:\n",
      "\n",
      "Verifying 3 random samples:\n",
      "\n",
      "Sample 618:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 16\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁applied             B-DU-NEG             2312       6         \n",
      "▁small               I-DU-NEG             536        47        \n",
      "▁drop                I-DU-NEG             1954       47        \n",
      "▁eyes                B_BP                 1365       36        \n",
      "▁see                 O                    398        78        \n",
      "▁would               O                    338        78        \n",
      "▁cultivate           O                    16163      78        \n",
      "▁blown               O                    9654       78        \n",
      "▁by                  O                    293        78        \n",
      "▁tight               O                    3793       78        \n",
      "▁end                 O                    513        78        \n",
      "▁reduce              B-EF-POS             1684       11        \n",
      "▁puffy               B-VP-POS             40364      35        \n",
      "▁bags                I-VP-POS             3713       72        \n",
      "▁eyes                B_BP                 1365       36        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 17: [SEP] -> Label ID: -100\n",
      "Position 18: [PAD] -> Label ID: -100\n",
      "Position 19: [PAD] -> Label ID: -100\n",
      "Position 20: [PAD] -> Label ID: -100\n",
      "Position 21: [PAD] -> Label ID: -100\n",
      "Position 22: [PAD] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 519:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 22\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁overall             O                    1629       78        \n",
      "▁since               O                    515        78        \n",
      "▁would               O                    338        78        \n",
      "▁hard                O                    657        78        \n",
      "▁assess              O                    5215       78        \n",
      "▁whether             O                    786        78        \n",
      "▁not                 O                    298        78        \n",
      "▁serum               B_PT                 11009      39        \n",
      "▁actually            B-VP-NEG             675        33        \n",
      "▁works               I-VP-NEG             885        70        \n",
      "▁wrinkles            I-VP-NEG             14776      70        \n",
      "▁acne                I-VP-NEG             10280      70        \n",
      "▁spots               I-VP-NEG             4393       70        \n",
      "▁would               O                    338        78        \n",
      "▁suppose             O                    5827       78        \n",
      "▁evil                O                    3876       78        \n",
      "▁serum               B_PT                 11009      39        \n",
      "▁try                 O                    687        78        \n",
      "▁you                 O                    274        78        \n",
      "▁preserved           O                    10209      78        \n",
      "▁retinol             B_IN                 75437      38        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 25: [SEP] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 549:\n",
      "Sequence length: 212\n",
      "Number of actual labels (excluding -100): 19\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁im                  O                    6438       78        \n",
      "▁fair                O                    2335       78        \n",
      "▁skinned             I_BP                 44872      73        \n",
      "▁every               O                    469        78        \n",
      "▁pimple              I_BP                 72498      73        \n",
      "▁popped              O                    12895      78        \n",
      "▁un                  O                    1655       78        \n",
      "▁left                O                    595        78        \n",
      "▁lasting             O                    5927       78        \n",
      "▁pink                O                    3782       78        \n",
      "▁scar                O                    16088      78        \n",
      "▁face                I_BP                 812        73        \n",
      "▁requiring           O                    6341       78        \n",
      "▁concealer           B_IN                 39299      38        \n",
      "▁months              O                    740        78        \n",
      "▁long                B-DU-POS             455        8         \n",
      "▁prescribed          O                    8048       78        \n",
      "▁divergence          O                    30188      78        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 22: [SEP] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "Position 211: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "TEST DATASET SAMPLES:\n",
      "\n",
      "Verifying 3 random samples:\n",
      "\n",
      "Sample 1537:\n",
      "Sequence length: 211\n",
      "Number of actual labels (excluding -100): 10\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁come                O                    488        78        \n",
      "▁hyaluronic          B_IN                 52869      38        \n",
      "▁acid                I_IN                 3584       75        \n",
      "▁niacin              B_IN                 66712      38        \n",
      "▁fer                 B_IN                 27397      38        \n",
      "▁acid                I_IN                 3584       75        \n",
      "▁com                 O                    6215       78        \n",
      "▁milky               B-TX-NEU             47948      31        \n",
      "▁serum               B_PT                 11009      39        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 14: [SEP] -> Label ID: -100\n",
      "Position 15: [PAD] -> Label ID: -100\n",
      "Position 16: [PAD] -> Label ID: -100\n",
      "Position 17: [PAD] -> Label ID: -100\n",
      "Position 18: [PAD] -> Label ID: -100\n",
      "Position 19: [PAD] -> Label ID: -100\n",
      "Position 20: [PAD] -> Label ID: -100\n",
      "Position 21: [PAD] -> Label ID: -100\n",
      "Position 22: [PAD] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 543:\n",
      "Sequence length: 211\n",
      "Number of actual labels (excluding -100): 21\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁protecting          O                    5582       78        \n",
      "▁skin                B_BP                 1158       36        \n",
      "▁sun                 O                    2119       78        \n",
      "▁non                 O                    745        78        \n",
      "▁yet                 O                    729        78        \n",
      "▁dont                O                    6421       78        \n",
      "▁give                O                    527        78        \n",
      "▁greasy              B-TX-NEG             25930      30        \n",
      "▁sunscreen           B_PT                 18839      39        \n",
      "▁lotions             O                    38973      78        \n",
      "▁second              O                    567        78        \n",
      "▁bottle              B-PK-NEG             3477       18        \n",
      "▁even                O                    402        78        \n",
      "▁price               B-PE-NEG             710        15        \n",
      "▁outside             O                    954        78        \n",
      "▁comfort             O                    2352       78        \n",
      "▁level               O                    674        78        \n",
      "▁feel                O                    551        78        \n",
      "▁entirely            O                    2945       78        \n",
      "▁worth               B-PE-NEG             1231       15        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 25: [SEP] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sample 1721:\n",
      "Sequence length: 211\n",
      "Number of actual labels (excluding -100): 7\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁hopelessly          O                    43486      78        \n",
      "▁amazon              I_BR                 12942      74        \n",
      "▁end                 O                    513        78        \n",
      "▁fragrance           I-SC-NEG             11649      61        \n",
      "▁barren              O                    29028      78        \n",
      "▁variety             O                    1261       78        \n",
      "▁.                   O                    323        78        \n",
      "\n",
      "Special tokens verification:\n",
      "Position 0: [CLS] -> Label ID: -100\n",
      "Position 8: [SEP] -> Label ID: -100\n",
      "Position 9: [PAD] -> Label ID: -100\n",
      "Position 10: [PAD] -> Label ID: -100\n",
      "Position 11: [PAD] -> Label ID: -100\n",
      "Position 12: [PAD] -> Label ID: -100\n",
      "Position 13: [PAD] -> Label ID: -100\n",
      "Position 14: [PAD] -> Label ID: -100\n",
      "Position 15: [PAD] -> Label ID: -100\n",
      "Position 16: [PAD] -> Label ID: -100\n",
      "Position 17: [PAD] -> Label ID: -100\n",
      "Position 18: [PAD] -> Label ID: -100\n",
      "Position 19: [PAD] -> Label ID: -100\n",
      "Position 20: [PAD] -> Label ID: -100\n",
      "Position 21: [PAD] -> Label ID: -100\n",
      "Position 22: [PAD] -> Label ID: -100\n",
      "Position 23: [PAD] -> Label ID: -100\n",
      "Position 24: [PAD] -> Label ID: -100\n",
      "Position 25: [PAD] -> Label ID: -100\n",
      "Position 26: [PAD] -> Label ID: -100\n",
      "Position 27: [PAD] -> Label ID: -100\n",
      "Position 28: [PAD] -> Label ID: -100\n",
      "Position 29: [PAD] -> Label ID: -100\n",
      "Position 30: [PAD] -> Label ID: -100\n",
      "Position 31: [PAD] -> Label ID: -100\n",
      "Position 32: [PAD] -> Label ID: -100\n",
      "Position 33: [PAD] -> Label ID: -100\n",
      "Position 34: [PAD] -> Label ID: -100\n",
      "Position 35: [PAD] -> Label ID: -100\n",
      "Position 36: [PAD] -> Label ID: -100\n",
      "Position 37: [PAD] -> Label ID: -100\n",
      "Position 38: [PAD] -> Label ID: -100\n",
      "Position 39: [PAD] -> Label ID: -100\n",
      "Position 40: [PAD] -> Label ID: -100\n",
      "Position 41: [PAD] -> Label ID: -100\n",
      "Position 42: [PAD] -> Label ID: -100\n",
      "Position 43: [PAD] -> Label ID: -100\n",
      "Position 44: [PAD] -> Label ID: -100\n",
      "Position 45: [PAD] -> Label ID: -100\n",
      "Position 46: [PAD] -> Label ID: -100\n",
      "Position 47: [PAD] -> Label ID: -100\n",
      "Position 48: [PAD] -> Label ID: -100\n",
      "Position 49: [PAD] -> Label ID: -100\n",
      "Position 50: [PAD] -> Label ID: -100\n",
      "Position 51: [PAD] -> Label ID: -100\n",
      "Position 52: [PAD] -> Label ID: -100\n",
      "Position 53: [PAD] -> Label ID: -100\n",
      "Position 54: [PAD] -> Label ID: -100\n",
      "Position 55: [PAD] -> Label ID: -100\n",
      "Position 56: [PAD] -> Label ID: -100\n",
      "Position 57: [PAD] -> Label ID: -100\n",
      "Position 58: [PAD] -> Label ID: -100\n",
      "Position 59: [PAD] -> Label ID: -100\n",
      "Position 60: [PAD] -> Label ID: -100\n",
      "Position 61: [PAD] -> Label ID: -100\n",
      "Position 62: [PAD] -> Label ID: -100\n",
      "Position 63: [PAD] -> Label ID: -100\n",
      "Position 64: [PAD] -> Label ID: -100\n",
      "Position 65: [PAD] -> Label ID: -100\n",
      "Position 66: [PAD] -> Label ID: -100\n",
      "Position 67: [PAD] -> Label ID: -100\n",
      "Position 68: [PAD] -> Label ID: -100\n",
      "Position 69: [PAD] -> Label ID: -100\n",
      "Position 70: [PAD] -> Label ID: -100\n",
      "Position 71: [PAD] -> Label ID: -100\n",
      "Position 72: [PAD] -> Label ID: -100\n",
      "Position 73: [PAD] -> Label ID: -100\n",
      "Position 74: [PAD] -> Label ID: -100\n",
      "Position 75: [PAD] -> Label ID: -100\n",
      "Position 76: [PAD] -> Label ID: -100\n",
      "Position 77: [PAD] -> Label ID: -100\n",
      "Position 78: [PAD] -> Label ID: -100\n",
      "Position 79: [PAD] -> Label ID: -100\n",
      "Position 80: [PAD] -> Label ID: -100\n",
      "Position 81: [PAD] -> Label ID: -100\n",
      "Position 82: [PAD] -> Label ID: -100\n",
      "Position 83: [PAD] -> Label ID: -100\n",
      "Position 84: [PAD] -> Label ID: -100\n",
      "Position 85: [PAD] -> Label ID: -100\n",
      "Position 86: [PAD] -> Label ID: -100\n",
      "Position 87: [PAD] -> Label ID: -100\n",
      "Position 88: [PAD] -> Label ID: -100\n",
      "Position 89: [PAD] -> Label ID: -100\n",
      "Position 90: [PAD] -> Label ID: -100\n",
      "Position 91: [PAD] -> Label ID: -100\n",
      "Position 92: [PAD] -> Label ID: -100\n",
      "Position 93: [PAD] -> Label ID: -100\n",
      "Position 94: [PAD] -> Label ID: -100\n",
      "Position 95: [PAD] -> Label ID: -100\n",
      "Position 96: [PAD] -> Label ID: -100\n",
      "Position 97: [PAD] -> Label ID: -100\n",
      "Position 98: [PAD] -> Label ID: -100\n",
      "Position 99: [PAD] -> Label ID: -100\n",
      "Position 100: [PAD] -> Label ID: -100\n",
      "Position 101: [PAD] -> Label ID: -100\n",
      "Position 102: [PAD] -> Label ID: -100\n",
      "Position 103: [PAD] -> Label ID: -100\n",
      "Position 104: [PAD] -> Label ID: -100\n",
      "Position 105: [PAD] -> Label ID: -100\n",
      "Position 106: [PAD] -> Label ID: -100\n",
      "Position 107: [PAD] -> Label ID: -100\n",
      "Position 108: [PAD] -> Label ID: -100\n",
      "Position 109: [PAD] -> Label ID: -100\n",
      "Position 110: [PAD] -> Label ID: -100\n",
      "Position 111: [PAD] -> Label ID: -100\n",
      "Position 112: [PAD] -> Label ID: -100\n",
      "Position 113: [PAD] -> Label ID: -100\n",
      "Position 114: [PAD] -> Label ID: -100\n",
      "Position 115: [PAD] -> Label ID: -100\n",
      "Position 116: [PAD] -> Label ID: -100\n",
      "Position 117: [PAD] -> Label ID: -100\n",
      "Position 118: [PAD] -> Label ID: -100\n",
      "Position 119: [PAD] -> Label ID: -100\n",
      "Position 120: [PAD] -> Label ID: -100\n",
      "Position 121: [PAD] -> Label ID: -100\n",
      "Position 122: [PAD] -> Label ID: -100\n",
      "Position 123: [PAD] -> Label ID: -100\n",
      "Position 124: [PAD] -> Label ID: -100\n",
      "Position 125: [PAD] -> Label ID: -100\n",
      "Position 126: [PAD] -> Label ID: -100\n",
      "Position 127: [PAD] -> Label ID: -100\n",
      "Position 128: [PAD] -> Label ID: -100\n",
      "Position 129: [PAD] -> Label ID: -100\n",
      "Position 130: [PAD] -> Label ID: -100\n",
      "Position 131: [PAD] -> Label ID: -100\n",
      "Position 132: [PAD] -> Label ID: -100\n",
      "Position 133: [PAD] -> Label ID: -100\n",
      "Position 134: [PAD] -> Label ID: -100\n",
      "Position 135: [PAD] -> Label ID: -100\n",
      "Position 136: [PAD] -> Label ID: -100\n",
      "Position 137: [PAD] -> Label ID: -100\n",
      "Position 138: [PAD] -> Label ID: -100\n",
      "Position 139: [PAD] -> Label ID: -100\n",
      "Position 140: [PAD] -> Label ID: -100\n",
      "Position 141: [PAD] -> Label ID: -100\n",
      "Position 142: [PAD] -> Label ID: -100\n",
      "Position 143: [PAD] -> Label ID: -100\n",
      "Position 144: [PAD] -> Label ID: -100\n",
      "Position 145: [PAD] -> Label ID: -100\n",
      "Position 146: [PAD] -> Label ID: -100\n",
      "Position 147: [PAD] -> Label ID: -100\n",
      "Position 148: [PAD] -> Label ID: -100\n",
      "Position 149: [PAD] -> Label ID: -100\n",
      "Position 150: [PAD] -> Label ID: -100\n",
      "Position 151: [PAD] -> Label ID: -100\n",
      "Position 152: [PAD] -> Label ID: -100\n",
      "Position 153: [PAD] -> Label ID: -100\n",
      "Position 154: [PAD] -> Label ID: -100\n",
      "Position 155: [PAD] -> Label ID: -100\n",
      "Position 156: [PAD] -> Label ID: -100\n",
      "Position 157: [PAD] -> Label ID: -100\n",
      "Position 158: [PAD] -> Label ID: -100\n",
      "Position 159: [PAD] -> Label ID: -100\n",
      "Position 160: [PAD] -> Label ID: -100\n",
      "Position 161: [PAD] -> Label ID: -100\n",
      "Position 162: [PAD] -> Label ID: -100\n",
      "Position 163: [PAD] -> Label ID: -100\n",
      "Position 164: [PAD] -> Label ID: -100\n",
      "Position 165: [PAD] -> Label ID: -100\n",
      "Position 166: [PAD] -> Label ID: -100\n",
      "Position 167: [PAD] -> Label ID: -100\n",
      "Position 168: [PAD] -> Label ID: -100\n",
      "Position 169: [PAD] -> Label ID: -100\n",
      "Position 170: [PAD] -> Label ID: -100\n",
      "Position 171: [PAD] -> Label ID: -100\n",
      "Position 172: [PAD] -> Label ID: -100\n",
      "Position 173: [PAD] -> Label ID: -100\n",
      "Position 174: [PAD] -> Label ID: -100\n",
      "Position 175: [PAD] -> Label ID: -100\n",
      "Position 176: [PAD] -> Label ID: -100\n",
      "Position 177: [PAD] -> Label ID: -100\n",
      "Position 178: [PAD] -> Label ID: -100\n",
      "Position 179: [PAD] -> Label ID: -100\n",
      "Position 180: [PAD] -> Label ID: -100\n",
      "Position 181: [PAD] -> Label ID: -100\n",
      "Position 182: [PAD] -> Label ID: -100\n",
      "Position 183: [PAD] -> Label ID: -100\n",
      "Position 184: [PAD] -> Label ID: -100\n",
      "Position 185: [PAD] -> Label ID: -100\n",
      "Position 186: [PAD] -> Label ID: -100\n",
      "Position 187: [PAD] -> Label ID: -100\n",
      "Position 188: [PAD] -> Label ID: -100\n",
      "Position 189: [PAD] -> Label ID: -100\n",
      "Position 190: [PAD] -> Label ID: -100\n",
      "Position 191: [PAD] -> Label ID: -100\n",
      "Position 192: [PAD] -> Label ID: -100\n",
      "Position 193: [PAD] -> Label ID: -100\n",
      "Position 194: [PAD] -> Label ID: -100\n",
      "Position 195: [PAD] -> Label ID: -100\n",
      "Position 196: [PAD] -> Label ID: -100\n",
      "Position 197: [PAD] -> Label ID: -100\n",
      "Position 198: [PAD] -> Label ID: -100\n",
      "Position 199: [PAD] -> Label ID: -100\n",
      "Position 200: [PAD] -> Label ID: -100\n",
      "Position 201: [PAD] -> Label ID: -100\n",
      "Position 202: [PAD] -> Label ID: -100\n",
      "Position 203: [PAD] -> Label ID: -100\n",
      "Position 204: [PAD] -> Label ID: -100\n",
      "Position 205: [PAD] -> Label ID: -100\n",
      "Position 206: [PAD] -> Label ID: -100\n",
      "Position 207: [PAD] -> Label ID: -100\n",
      "Position 208: [PAD] -> Label ID: -100\n",
      "Position 209: [PAD] -> Label ID: -100\n",
      "Position 210: [PAD] -> Label ID: -100\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ANALYZING LABEL DISTRIBUTIONS:\n",
      "\n",
      "Training Set:\n",
      "\n",
      "Label Distribution:\n",
      "--------------------------------------------------\n",
      "Label                          Count      Percentage\n",
      "--------------------------------------------------\n",
      "O                              124836      71.45%\n",
      "B_PT                           5941         3.40%\n",
      "B_BP                           5120         2.93%\n",
      "I_PT                           4824         2.76%\n",
      "B_IN                           2773         1.59%\n",
      "I_BP                           2609         1.49%\n",
      "I_IN                           1816         1.04%\n",
      "I_BR                           1573         0.90%\n",
      "B_BR                           1437         0.82%\n",
      "B-TX-NEG                       1287         0.74%\n",
      "B-VP-NEG                       1280         0.73%\n",
      "B-VP-POS                       1108         0.63%\n",
      "I-SC-NEG                       1043         0.60%\n",
      "I-PK-NEG                       905          0.52%\n",
      "I-VP-NEG                       905          0.52%\n",
      "B-PK-NEG                       883          0.51%\n",
      "B-SC-NEG                       783          0.45%\n",
      "B-TX-POS                       714          0.41%\n",
      "B-HY-POS                       712          0.41%\n",
      "B-HY-NEG                       702          0.40%\n",
      "B-EF-NEG                       644          0.37%\n",
      "I-SC-POS                       644          0.37%\n",
      "I-VP-POS                       592          0.34%\n",
      "B-SE-NEG                       573          0.33%\n",
      "I-TX-NEG                       547          0.31%\n",
      "B-PE-POS                       544          0.31%\n",
      "B-SE-POS                       524          0.30%\n",
      "B-PE-NEG                       510          0.29%\n",
      "B-AP-NEG                       501          0.29%\n",
      "B-PK-POS                       480          0.27%\n",
      "I-SE-NEG                       446          0.26%\n",
      "I-PK-POS                       431          0.25%\n",
      "B-AB-NEG                       416          0.24%\n",
      "B-SC-POS                       412          0.24%\n",
      "B-QU-NEG                       385          0.22%\n",
      "I-EF-NEG                       379          0.22%\n",
      "B-EF-POS                       373          0.21%\n",
      "B-DU-NEG                       369          0.21%\n",
      "B_SW                           304          0.17%\n",
      "I-TX-POS                       288          0.16%\n",
      "B-AP-POS                       279          0.16%\n",
      "I-EF-POS                       228          0.13%\n",
      "I-SE-POS                       226          0.13%\n",
      "I-HY-POS                       221          0.13%\n",
      "B-VP-NEU                       220          0.13%\n",
      "I-HY-NEG                       212          0.12%\n",
      "B-PK-NEU                       204          0.12%\n",
      "B-AB-POS                       197          0.11%\n",
      "I-PE-POS                       182          0.10%\n",
      "I-PE-NEG                       176          0.10%\n",
      "B-QU-POS                       167          0.10%\n",
      "I-QU-NEG                       165          0.09%\n",
      "I-AB-NEG                       165          0.09%\n",
      "I-AP-NEG                       149          0.09%\n",
      "B-DU-POS                       148          0.08%\n",
      "I-DU-NEG                       128          0.07%\n",
      "B-SE-NEU                       107          0.06%\n",
      "B-SC-NEU                       97           0.06%\n",
      "I-AB-POS                       94           0.05%\n",
      "I-QU-POS                       92           0.05%\n",
      "I-VP-NEU                       88           0.05%\n",
      "I-AP-POS                       83           0.05%\n",
      "I_SW                           66           0.04%\n",
      "B-EF-NEU                       57           0.03%\n",
      "B-TX-NEU                       53           0.03%\n",
      "B-AP-NEU                       43           0.02%\n",
      "I-DU-POS                       42           0.02%\n",
      "I-SC-NEU                       40           0.02%\n",
      "B-QU-NEU                       38           0.02%\n",
      "B-PE-NEU                       32           0.02%\n",
      "B-HY-NEU                       27           0.02%\n",
      "I-AP-NEU                       20           0.01%\n",
      "I-TX-NEU                       17           0.01%\n",
      "B-AB-NEU                       11           0.01%\n",
      "I-DU-NEU                       9            0.01%\n",
      "I-AB-NEU                       8            0.00%\n",
      "B-DU-NEU                       7            0.00%\n",
      "I-SE-NEU                       4            0.00%\n",
      "I-EF-NEU                       4            0.00%\n",
      "\n",
      "Validation Set:\n",
      "\n",
      "Label Distribution:\n",
      "--------------------------------------------------\n",
      "Label                          Count      Percentage\n",
      "--------------------------------------------------\n",
      "O                              27032       71.31%\n",
      "B_PT                           1325         3.50%\n",
      "B_BP                           1134         2.99%\n",
      "I_PT                           1022         2.70%\n",
      "B_IN                           635          1.68%\n",
      "I_BP                           518          1.37%\n",
      "I_IN                           404          1.07%\n",
      "I_BR                           374          0.99%\n",
      "B_BR                           345          0.91%\n",
      "B-TX-NEG                       267          0.70%\n",
      "I-SC-NEG                       229          0.60%\n",
      "B-VP-NEG                       227          0.60%\n",
      "I-PK-NEG                       208          0.55%\n",
      "B-VP-POS                       196          0.52%\n",
      "B-PK-NEG                       194          0.51%\n",
      "B-SC-NEG                       173          0.46%\n",
      "B-HY-NEG                       170          0.45%\n",
      "B-EF-NEG                       168          0.44%\n",
      "B-TX-POS                       159          0.42%\n",
      "B-HY-POS                       159          0.42%\n",
      "I-VP-NEG                       157          0.41%\n",
      "I-SC-POS                       149          0.39%\n",
      "B-SE-NEG                       148          0.39%\n",
      "I-PK-POS                       125          0.33%\n",
      "B-SC-POS                       118          0.31%\n",
      "B-PE-NEG                       118          0.31%\n",
      "B-PE-POS                       117          0.31%\n",
      "B-PK-POS                       114          0.30%\n",
      "I-SE-NEG                       107          0.28%\n",
      "I-VP-POS                       107          0.28%\n",
      "B-AB-NEG                       103          0.27%\n",
      "B-SE-POS                       101          0.27%\n",
      "I-TX-NEG                       95           0.25%\n",
      "B-AP-NEG                       90           0.24%\n",
      "B-EF-POS                       81           0.21%\n",
      "B_SW                           80           0.21%\n",
      "I-EF-NEG                       78           0.21%\n",
      "B-DU-NEG                       72           0.19%\n",
      "I-TX-POS                       71           0.19%\n",
      "B-AP-POS                       64           0.17%\n",
      "B-QU-NEG                       63           0.17%\n",
      "I-HY-POS                       58           0.15%\n",
      "B-VP-NEU                       53           0.14%\n",
      "I-SE-POS                       53           0.14%\n",
      "I-QU-NEG                       52           0.14%\n",
      "I-EF-POS                       47           0.12%\n",
      "B-PK-NEU                       46           0.12%\n",
      "I-HY-NEG                       44           0.12%\n",
      "B-AB-POS                       36           0.09%\n",
      "B-QU-POS                       35           0.09%\n",
      "I-AP-POS                       34           0.09%\n",
      "I-AB-NEG                       33           0.09%\n",
      "I-DU-NEG                       33           0.09%\n",
      "B-SE-NEU                       31           0.08%\n",
      "I-PE-POS                       30           0.08%\n",
      "I-AP-NEG                       25           0.07%\n",
      "B-DU-POS                       22           0.06%\n",
      "I-PE-NEG                       22           0.06%\n",
      "I-AB-POS                       20           0.05%\n",
      "B-SC-NEU                       17           0.04%\n",
      "I-VP-NEU                       15           0.04%\n",
      "I-QU-POS                       11           0.03%\n",
      "I-DU-POS                       11           0.03%\n",
      "B-TX-NEU                       11           0.03%\n",
      "B-AP-NEU                       10           0.03%\n",
      "B-QU-NEU                       9            0.02%\n",
      "B-EF-NEU                       9            0.02%\n",
      "B-HY-NEU                       6            0.02%\n",
      "I_SW                           6            0.02%\n",
      "I-TX-NEU                       5            0.01%\n",
      "I-DU-NEU                       5            0.01%\n",
      "I-SC-NEU                       5            0.01%\n",
      "B-AB-NEU                       4            0.01%\n",
      "I-AB-NEU                       4            0.01%\n",
      "B-DU-NEU                       3            0.01%\n",
      "B-PE-NEU                       2            0.01%\n",
      "I-AP-NEU                       2            0.01%\n",
      "I-EF-NEU                       1            0.00%\n",
      "\n",
      "Test Set:\n",
      "\n",
      "Label Distribution:\n",
      "--------------------------------------------------\n",
      "Label                          Count      Percentage\n",
      "--------------------------------------------------\n",
      "O                              26270       71.61%\n",
      "B_PT                           1167         3.18%\n",
      "I_PT                           1060         2.89%\n",
      "B_BP                           1052         2.87%\n",
      "I_BP                           610          1.66%\n",
      "B_IN                           542          1.48%\n",
      "I_IN                           357          0.97%\n",
      "B_BR                           306          0.83%\n",
      "I_BR                           273          0.74%\n",
      "B-VP-NEG                       256          0.70%\n",
      "B-TX-NEG                       246          0.67%\n",
      "B-VP-POS                       210          0.57%\n",
      "I-VP-NEG                       209          0.57%\n",
      "I-SC-NEG                       196          0.53%\n",
      "B-PK-NEG                       176          0.48%\n",
      "B-HY-POS                       169          0.46%\n",
      "I-PK-NEG                       168          0.46%\n",
      "B-TX-POS                       165          0.45%\n",
      "B-SC-NEG                       159          0.43%\n",
      "B-HY-NEG                       152          0.41%\n",
      "B-SE-NEG                       146          0.40%\n",
      "I-SC-POS                       144          0.39%\n",
      "B-EF-NEG                       129          0.35%\n",
      "I-TX-NEG                       127          0.35%\n",
      "I-VP-POS                       119          0.32%\n",
      "B-PE-NEG                       119          0.32%\n",
      "B-SE-POS                       118          0.32%\n",
      "I-PK-POS                       117          0.32%\n",
      "B-SC-POS                       111          0.30%\n",
      "B-PK-POS                       110          0.30%\n",
      "B-PE-POS                       110          0.30%\n",
      "B-EF-POS                       99           0.27%\n",
      "B-QU-NEG                       91           0.25%\n",
      "B-AP-NEG                       90           0.25%\n",
      "I-SE-NEG                       90           0.25%\n",
      "I-EF-NEG                       78           0.21%\n",
      "B_SW                           76           0.21%\n",
      "B-AB-NEG                       74           0.20%\n",
      "B-AP-POS                       66           0.18%\n",
      "B-DU-NEG                       65           0.18%\n",
      "I-TX-POS                       58           0.16%\n",
      "I-EF-POS                       53           0.14%\n",
      "I-HY-POS                       50           0.14%\n",
      "I-HY-NEG                       49           0.13%\n",
      "I-SE-POS                       48           0.13%\n",
      "I-QU-NEG                       42           0.11%\n",
      "B-AB-POS                       41           0.11%\n",
      "B-PK-NEU                       40           0.11%\n",
      "B-VP-NEU                       37           0.10%\n",
      "I-PE-POS                       37           0.10%\n",
      "B-QU-POS                       37           0.10%\n",
      "B-DU-POS                       36           0.10%\n",
      "I-AB-NEG                       31           0.08%\n",
      "B-SE-NEU                       29           0.08%\n",
      "I-DU-NEG                       26           0.07%\n",
      "I-QU-POS                       25           0.07%\n",
      "B-SC-NEU                       24           0.07%\n",
      "I-AP-POS                       24           0.07%\n",
      "I-AP-NEG                       21           0.06%\n",
      "I-PE-NEG                       20           0.05%\n",
      "I-AB-POS                       20           0.05%\n",
      "I_SW                           14           0.04%\n",
      "B-TX-NEU                       14           0.04%\n",
      "I-DU-POS                       13           0.04%\n",
      "I-VP-NEU                       13           0.04%\n",
      "B-EF-NEU                       11           0.03%\n",
      "B-PE-NEU                       8            0.02%\n",
      "I-SC-NEU                       8            0.02%\n",
      "B-QU-NEU                       7            0.02%\n",
      "B-AB-NEU                       6            0.02%\n",
      "I-DU-NEU                       4            0.01%\n",
      "B-HY-NEU                       3            0.01%\n",
      "I-AB-NEU                       3            0.01%\n",
      "I-TX-NEU                       2            0.01%\n",
      "I-SE-NEU                       2            0.01%\n",
      "B-AP-NEU                       2            0.01%\n",
      "I-AP-NEU                       2            0.01%\n",
      "B-DU-NEU                       2            0.01%\n"
     ]
    }
   ],
   "source": [
    "def verify_samples(dataset, tokenizer, id2label, num_samples=5):\n",
    "    \"\"\"\n",
    "    Verify multiple samples from the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\nVerifying {num_samples} random samples:\")\n",
    "    \n",
    "    # Get random indices\n",
    "    indices = np.random.randint(0, len(dataset), num_samples)\n",
    "    \n",
    "    for idx in indices:\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        sample = dataset[idx]\n",
    "        \n",
    "        # Get original tokens and labels\n",
    "        tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
    "        labels = sample['labels']\n",
    "        \n",
    "        # Print sequence information\n",
    "        print(f\"Sequence length: {len(tokens)}\")\n",
    "        print(f\"Number of actual labels (excluding -100): {sum(1 for l in labels if l != -100)}\")\n",
    "        \n",
    "        print(\"\\nToken-Label Alignment:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Token':<20} {'Label':<20} {'Token ID':<10} {'Label ID':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for token, label_id, token_id in zip(tokens, labels, sample['input_ids']):\n",
    "            if label_id != -100:  # Only show non-special tokens\n",
    "                label = id2label[label_id.item()]\n",
    "                print(f\"{token:<20} {label:<20} {token_id:<10} {label_id:<10}\")\n",
    "        \n",
    "        # Verify special tokens\n",
    "        special_tokens = [\n",
    "            (i, token) for i, token in enumerate(tokens) \n",
    "            if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]\n",
    "        ]\n",
    "        print(\"\\nSpecial tokens verification:\")\n",
    "        for pos, token in special_tokens:\n",
    "            print(f\"Position {pos}: {token} -> Label ID: {labels[pos].item()}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Verify samples from each dataset\n",
    "print(\"\\nTRAINING DATASET SAMPLES:\")\n",
    "verify_samples(train_dataset, tokenizer, id2label, num_samples=3)\n",
    "\n",
    "print(\"\\nVALIDATION DATASET SAMPLES:\")\n",
    "verify_samples(val_dataset, tokenizer, id2label, num_samples=3)\n",
    "\n",
    "print(\"\\nTEST DATASET SAMPLES:\")\n",
    "verify_samples(test_dataset, tokenizer, id2label, num_samples=3)\n",
    "\n",
    "# Additional distribution analysis\n",
    "def analyze_label_distribution(dataset, id2label):\n",
    "    label_counts = {}\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        labels = dataset[i]['labels']\n",
    "        for label in labels:\n",
    "            if label != -100:\n",
    "                label_name = id2label[label.item()]\n",
    "                label_counts[label_name] = label_counts.get(label_name, 0) + 1\n",
    "                total_tokens += 1\n",
    "    \n",
    "    print(\"\\nLabel Distribution:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Label':<30} {'Count':<10} {'Percentage':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for label, count in sorted(label_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / total_tokens) * 100\n",
    "        print(f\"{label:<30} {count:<10} {percentage:>6.2f}%\")\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "print(\"\\nANALYZING LABEL DISTRIBUTIONS:\")\n",
    "print(\"\\nTraining Set:\")\n",
    "train_dist = analyze_label_distribution(train_dataset, id2label)\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "val_dist = analyze_label_distribution(val_dataset, id2label)  # Fixed here\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "test_dist = analyze_label_distribution(test_dataset, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a36cfd-d24e-42f5-b9ca-7f9b15cd3373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:39:05.245596Z",
     "iopub.status.busy": "2024-12-30T07:39:05.245400Z",
     "iopub.status.idle": "2024-12-30T07:39:07.437846Z",
     "shell.execute_reply": "2024-12-30T07:39:07.437255Z",
     "shell.execute_reply.started": "2024-12-30T07:39:05.245581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 4433 sequences with incorrect BIO scheme\n",
      "\n",
      "Comparing original and fixed labels:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Tokens: ['downside', 'shade', 'range', 'lightest', 'still', 'slightly', 'darker', 'natural', 'color', 'not', 'deal', 'breaker', '.']\n",
      "Original: ['O', 'B-VP-NEG', 'I-VP-NEG', 'I-VP-NEG', 'O', 'B-AP-NEG', 'I-AP-NEG', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Fixed:    ['O', 'B-VP-NEG', 'I-VP-NEG', 'B-VP-NEG', 'O', 'B-AP-NEG', 'I-AP-NEG', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Example 13:\n",
      "Tokens: ['research', 'done', 'company', 'claim', 'high', 'spf', 'ppd', 'rating', 'used', 'european', 'standards', 'test', 'better', 'us', '.']\n",
      "Original: ['O', 'O', 'O', 'O', 'B-EF-POS', 'I-EF-POS', 'I-EF-POS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Fixed:    ['O', 'O', 'O', 'O', 'B-EF-POS', 'I-EF-POS', 'B-EF-POS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "----------------------------------------------------------------------\n",
      "... more examples exist ...\n",
      "\n",
      "Verifying a sample after BIO scheme fix:\n",
      "\n",
      "Token-Label Alignment:\n",
      "--------------------------------------------------\n",
      "Token                Label                Token ID   Label ID  \n",
      "--------------------------------------------------\n",
      "▁love                B-PK-POS             472        20        \n",
      "▁product             B_PT                 714        39        \n",
      "▁look                O                    468        78        \n",
      "▁working             O                    560        78        \n",
      "▁.                   O                    323        78        \n"
     ]
    }
   ],
   "source": [
    "def fix_bio_scheme(df):\n",
    "    \"\"\"\n",
    "    Fix BIO scheme consistency:\n",
    "    - Change I- tags to B- tags if they don't have a preceding B- tag\n",
    "    \"\"\"\n",
    "    fixed_count = 0\n",
    "    \n",
    "    def fix_sequence_labels(labels):\n",
    "        fixed = False\n",
    "        new_labels = labels.copy()\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            if label.startswith('I-'):\n",
    "                # Check if there's a matching B- tag before this I- tag\n",
    "                prefix = label[2:]  # Get the part after 'I-'\n",
    "                if i == 0 or not labels[i-1].startswith('B-') or not labels[i-1][2:] == prefix:\n",
    "                    new_labels[i] = 'B-' + prefix\n",
    "                    fixed = True\n",
    "                    \n",
    "        return new_labels, fixed\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    fixed_df = df.copy()\n",
    "    \n",
    "    # Fix labels for each row\n",
    "    for idx, row in fixed_df.iterrows():\n",
    "        new_labels, was_fixed = fix_sequence_labels(row['labels'])\n",
    "        if was_fixed:\n",
    "            fixed_count += 1\n",
    "            fixed_df.at[idx, 'labels'] = new_labels\n",
    "    \n",
    "    print(f\"Fixed {fixed_count} sequences with incorrect BIO scheme\")\n",
    "    \n",
    "    return fixed_df\n",
    "\n",
    "# Fix the BIO scheme in processed_df\n",
    "fixed_processed_df = fix_bio_scheme(processed_df)\n",
    "\n",
    "# Let's verify a few examples where changes were made\n",
    "def compare_labels(original_df, fixed_df):\n",
    "    print(\"\\nComparing original and fixed labels:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx in range(len(original_df)):\n",
    "        orig_labels = original_df.iloc[idx]['labels']\n",
    "        fixed_labels = fixed_df.iloc[idx]['labels']\n",
    "        \n",
    "        if orig_labels != fixed_labels:\n",
    "            print(f\"\\nExample {idx}:\")\n",
    "            print(\"Tokens:\", original_df.iloc[idx]['tokens'])\n",
    "            print(\"Original:\", orig_labels)\n",
    "            print(\"Fixed:   \", fixed_labels)\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            # Only show first 5 examples\n",
    "            if idx >= 4:\n",
    "                print(\"... more examples exist ...\")\n",
    "                break\n",
    "\n",
    "# Compare original and fixed labels\n",
    "compare_labels(processed_df, fixed_processed_df)\n",
    "\n",
    "# Use the fixed dataframe for further processing\n",
    "processed_df = fixed_processed_df\n",
    "\n",
    "# Re-run the dataset creation with fixed labels\n",
    "# Split the dataset\n",
    "train_df, temp_df = train_test_split(processed_df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create features for each split\n",
    "train_features = convert_to_features(train_df, tokenizer, label2id)\n",
    "val_features = convert_to_features(val_df, tokenizer, label2id)\n",
    "test_features = convert_to_features(test_df, tokenizer, label2id)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = ABSADataset(train_features)\n",
    "val_dataset = ABSADataset(val_features)\n",
    "test_dataset = ABSADataset(test_features)\n",
    "\n",
    "# Verify a sample after fixing\n",
    "print(\"\\nVerifying a sample after BIO scheme fix:\")\n",
    "sample_idx = 0\n",
    "sample = train_dataset[sample_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample['input_ids'])\n",
    "labels = sample['labels']\n",
    "\n",
    "print(\"\\nToken-Label Alignment:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Token':<20} {'Label':<20} {'Token ID':<10} {'Label ID':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for token, label_id, token_id in zip(tokens, labels, sample['input_ids']):\n",
    "    if label_id != -100:  # Only show non-special tokens\n",
    "        label = id2label[label_id.item()]\n",
    "        print(f\"{token:<20} {label:<20} {token_id:<10} {label_id:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90b0cd7c-f53e-460c-a349-67205c50a02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:53:54.221118Z",
     "iopub.status.busy": "2024-12-30T07:53:54.220859Z",
     "iopub.status.idle": "2024-12-30T07:53:58.433540Z",
     "shell.execute_reply": "2024-12-30T07:53:58.432895Z",
     "shell.execute_reply.started": "2024-12-30T07:53:54.221098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=287366df3f618fc6d7598b0ce15a83ca641ab51cc8a9f2e480a75c0819471910\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9a11e8b-298d-4dfc-990c-8e9f6d1d6193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T08:18:41.800781Z",
     "iopub.status.busy": "2024-12-30T08:18:41.800011Z",
     "iopub.status.idle": "2024-12-30T08:18:41.941700Z",
     "shell.execute_reply": "2024-12-30T08:18:41.941196Z",
     "shell.execute_reply.started": "2024-12-30T08:18:41.800781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to fixed_dataset.pkl\n",
      "Successfully loaded dataset from fixed_dataset.pkl\n",
      "Dataset size: 14946 rows\n",
      "\n",
      "Verifying loaded data:\n",
      "Number of rows: 14946\n",
      "\n",
      "Sample row:\n",
      "Tokens: ['use', 'instead', 'foundation', '.']\n",
      "Labels: ['O', 'O', 'B_PT', 'O']\n"
     ]
    }
   ],
   "source": [
    "'''def save_processed_data(df, filename='fixed_dataset.pkl'):\n",
    "    \"\"\"\n",
    "    Save the processed dataframe with all its structure intact\n",
    "    \"\"\"\n",
    "    df.to_pickle(filename)\n",
    "    print(f\"Dataset saved to {filename}\")'''\n",
    "\n",
    "def load_processed_data(filename='fixed_dataset.pkl'):\n",
    "    \"\"\"\n",
    "    Load the processed dataframe with all its structure intact\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_pickle(filename)\n",
    "        print(f\"Successfully loaded dataset from {filename}\")\n",
    "        print(f\"Dataset size: {len(df)} rows\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {filename} not found\")\n",
    "        return None\n",
    "\n",
    "# Save the fixed dataset\n",
    "save_processed_data(fixed_processed_df)\n",
    "\n",
    "# Later, you can load the dataset using:\n",
    "processed_df = load_processed_data()\n",
    "\n",
    "# Verify the loaded data\n",
    "if processed_df is not None:\n",
    "    print(\"\\nVerifying loaded data:\")\n",
    "    print(f\"Number of rows: {len(processed_df)}\")\n",
    "    print(\"\\nSample row:\")\n",
    "    sample_row = processed_df.iloc[0]\n",
    "    print(\"Tokens:\", sample_row['tokens'])\n",
    "    print(\"Labels:\", sample_row['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ab9746a-18eb-4a17-a924-0788c619d779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:54:29.800435Z",
     "iopub.status.busy": "2024-12-30T07:54:29.800187Z",
     "iopub.status.idle": "2024-12-30T07:54:31.202074Z",
     "shell.execute_reply": "2024-12-30T07:54:31.201591Z",
     "shell.execute_reply.started": "2024-12-30T07:54:29.800417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "GPU Model: NVIDIA RTX A4000\n",
      "Available GPU memory: 15.73 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Configuration:\n",
      "Training samples: 10462\n",
      "Validation samples: 2242\n",
      "Batch size: 8\n",
      "Number of epochs: 3\n",
      "Learning rate: 2e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Set up the model with GPU\n",
    "num_labels = len(label2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)  # Move model to GPU\n",
    "\n",
    "# First, install seqeval\n",
    "\n",
    "\n",
    "# Then update the training code:\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Convert our custom tags to standard NER format\n",
    "    def convert_to_ner_format(tags):\n",
    "        return [[tag.replace('_', '-') if tag != 'O' else tag for tag in seq] for seq in tags]\n",
    "\n",
    "    # Convert both predictions and labels to NER format\n",
    "    true_predictions = convert_to_ner_format(true_predictions)\n",
    "    true_labels = convert_to_ner_format(true_labels)\n",
    "\n",
    "    seqeval_metric = load_metric(\"seqeval\")\n",
    "    results = seqeval_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "\n",
    "# Define training arguments with GPU settings\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    no_cuda=False,\n",
    "    #fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    # Show training loss but no other logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Rest of the code remains the same\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Print training dataset size and batch size info\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Number of epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c26d3b3-cef6-4667-a98a-5de857458ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T07:54:35.073920Z",
     "iopub.status.busy": "2024-12-30T07:54:35.073367Z",
     "iopub.status.idle": "2024-12-30T08:04:11.245484Z",
     "shell.execute_reply": "2024-12-30T08:04:11.244996Z",
     "shell.execute_reply.started": "2024-12-30T07:54:35.073900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3924' max='3924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3924/3924 09:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321100</td>\n",
       "      <td>0.695545</td>\n",
       "      <td>0.732778</td>\n",
       "      <td>0.713676</td>\n",
       "      <td>0.910069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195990</td>\n",
       "      <td>0.781260</td>\n",
       "      <td>0.811506</td>\n",
       "      <td>0.796096</td>\n",
       "      <td>0.938323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169112</td>\n",
       "      <td>0.813114</td>\n",
       "      <td>0.832811</td>\n",
       "      <td>0.822844</td>\n",
       "      <td>0.946553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='562' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 04:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16911153495311737, 'eval_precision': 0.8131137155527399, 'eval_recall': 0.8328106412890667, 'eval_f1': 0.822844320974463, 'eval_accuracy': 0.9465534070224497, 'eval_runtime': 9.1219, 'eval_samples_per_second': 245.781, 'eval_steps_per_second': 30.805, 'epoch': 3.0}\n",
      "\n",
      "GPU Memory Summary:\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   5110 MiB |   7071 MiB |  56244 GiB |  56239 GiB |\n",
      "|       from large pool |   5105 MiB |   7035 MiB |  54905 GiB |  54900 GiB |\n",
      "|       from small pool |      5 MiB |     61 MiB |   1339 GiB |   1339 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   5110 MiB |   7071 MiB |  56244 GiB |  56239 GiB |\n",
      "|       from large pool |   5105 MiB |   7035 MiB |  54905 GiB |  54900 GiB |\n",
      "|       from small pool |      5 MiB |     61 MiB |   1339 GiB |   1339 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   5073 MiB |   6972 MiB |  55002 GiB |  54998 GiB |\n",
      "|       from large pool |   5068 MiB |   6935 MiB |  53663 GiB |  53658 GiB |\n",
      "|       from small pool |      5 MiB |     61 MiB |   1339 GiB |   1339 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   8048 MiB |   8048 MiB |  12572 MiB |   4524 MiB |\n",
      "|       from large pool |   7980 MiB |   7980 MiB |  12476 MiB |   4496 MiB |\n",
      "|       from small pool |     68 MiB |     68 MiB |     96 MiB |     28 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 574705 KiB |   1319 MiB |  49911 GiB |  49911 GiB |\n",
      "|       from large pool | 563476 KiB |   1304 MiB |  48433 GiB |  48432 GiB |\n",
      "|       from small pool |  11228 KiB |     16 MiB |   1478 GiB |   1478 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1424    |    1971    |   13127 K  |   13125 K  |\n",
      "|       from large pool |     522    |     877    |    8170 K  |    8170 K  |\n",
      "|       from small pool |     902    |    1160    |    4956 K  |    4955 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1424    |    1971    |   13127 K  |   13125 K  |\n",
      "|       from large pool |     522    |     877    |    8170 K  |    8170 K  |\n",
      "|       from small pool |     902    |    1160    |    4956 K  |    4955 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     233    |     233    |     369    |     136    |\n",
      "|       from large pool |     199    |     200    |     321    |     122    |\n",
      "|       from small pool |      34    |      34    |      48    |      14    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     108    |     164    |    7185 K  |    7185 K  |\n",
      "|       from large pool |      81    |     115    |    5164 K  |    5164 K  |\n",
      "|       from small pool |      27    |      72    |    2021 K  |    2021 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_IN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_PT seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BP seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_BR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: B_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: I_SW seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Print GPU memory usage after training\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nGPU Memory Summary:\")\n",
    "    print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9631d9-cfb2-46ba-a9f8-e056abe3a305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-30T08:08:39.568508Z",
     "iopub.status.busy": "2024-12-30T08:08:39.567952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "{'eval_loss': 0.1695667803287506, 'eval_precision': 0.8083361046447178, 'eval_recall': 0.8286363636363636, 'eval_f1': 0.8183603613714157, 'eval_accuracy': 0.9465161923454367, 'eval_runtime': 8.9462, 'eval_samples_per_second': 250.609, 'eval_steps_per_second': 31.41, 'epoch': 3.0}\n",
      "\n",
      "Testing predictions on example texts:\n",
      "\n",
      "Text: The sunscreen absorbs easily.\n",
      "Detected aspects and sentiments:\n",
      "- [CLS]: None\n",
      "- ▁absorbs ▁easily: AB-NEG\n",
      "- [SEP]: SE-NEU\n",
      "\n",
      "Text: The seal of the bottle was open but the moisturizer was good.\n",
      "Detected aspects and sentiments:\n",
      "- [CLS]: None\n",
      "- ▁bottle: PK-NEU\n",
      "- [SEP]: SE-NEU\n",
      "\n",
      "Performing error analysis...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "tensor(78)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Perform error analysis\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerforming error analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m errors \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(errors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m errors in sample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample of errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 83\u001b[0m, in \u001b[0;36manalyze_errors\u001b[0;34m(dataset, model, tokenizer, n_samples)\u001b[0m\n\u001b[1;32m     80\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict_aspects(text)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Get true labels\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compare predictions with true labels\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;241m!=\u001b[39m true_labels:  \u001b[38;5;66;03m# This is a simplified comparison\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 83\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predict_aspects(text)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Get true labels\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compare predictions with true labels\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;241m!=\u001b[39m true_labels:  \u001b[38;5;66;03m# This is a simplified comparison\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor(78)"
     ]
    }
   ],
   "source": [
    "def predict_aspects(text, model=model, tokenizer=tokenizer):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Convert predictions to labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predictions = predictions[0].cpu().numpy()\n",
    "    \n",
    "    # Align predictions with tokens\n",
    "    results = []\n",
    "    current_aspect = []\n",
    "    current_label = None\n",
    "    \n",
    "    for i, (token, pred_id) in enumerate(zip(tokens, predictions)):\n",
    "        # Skip special tokens and their predictions\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "            \n",
    "        if pred_id != -100:\n",
    "            try:\n",
    "                pred_label = id2label[pred_id]\n",
    "                \n",
    "                # Handle subword tokens (tokens starting with '▁' or '##')\n",
    "                cleaned_token = token.replace('▁', '').replace('##', '')\n",
    "                \n",
    "                if pred_label != 'O':\n",
    "                    if pred_label.startswith('B-'):\n",
    "                        # Save previous aspect if exists\n",
    "                        if current_aspect:\n",
    "                            results.append((' '.join(current_aspect), current_label))\n",
    "                        current_aspect = [cleaned_token]\n",
    "                        current_label = pred_label[2:]  # Remove B- prefix\n",
    "                    elif pred_label.startswith('I-'):\n",
    "                        if current_aspect:  # Only append if we have a current aspect\n",
    "                            current_aspect.append(cleaned_token)\n",
    "                else:\n",
    "                    # Save previous aspect if exists\n",
    "                    if current_aspect:\n",
    "                        results.append((' '.join(current_aspect), current_label))\n",
    "                        current_aspect = []\n",
    "                        current_label = None\n",
    "            except KeyError:\n",
    "                print(f\"Warning: Unknown label ID {pred_id}\")\n",
    "                continue\n",
    "    \n",
    "    # Add final aspect if exists\n",
    "    if current_aspect:\n",
    "        results.append((' '.join(current_aspect), current_label))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the updated function\n",
    "test_texts = [\n",
    "    \"The sunscreen absorbs easily.\",\n",
    "    \"The seal of the bottle was open but the moisturizer was good.\",\n",
    "    \"This cream has a strong fragrance but works well.\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting predictions on example texts:\")\n",
    "for text in test_texts:\n",
    "    print(\"\\nText:\", text)\n",
    "    aspects = predict_aspects(text)\n",
    "    print(\"Detected aspects and sentiments:\")\n",
    "    for aspect, sentiment in aspects:\n",
    "        print(f\"- {aspect}: {sentiment}\")\n",
    "\n",
    "# For error analysis, we need to modify the comparison function\n",
    "def analyze_errors(dataset, model, tokenizer, n_samples=50):\n",
    "    \"\"\"Analyze prediction errors on a sample of the dataset\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Get random sample indices\n",
    "    sample_indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        # Get sample\n",
    "        sample = dataset[idx]\n",
    "        text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "        \n",
    "        # Get predictions\n",
    "        pred_aspects = predict_aspects(text)\n",
    "        \n",
    "        # Get true labels (handle tensor values)\n",
    "        try:\n",
    "            true_labels = []\n",
    "            current_aspect = []\n",
    "            current_label = None\n",
    "            \n",
    "            for label in sample['labels']:\n",
    "                if label != -100:\n",
    "                    label_str = id2label[label.item()]  # Convert tensor to int\n",
    "                    if label_str.startswith('B-'):\n",
    "                        if current_aspect:\n",
    "                            true_labels.append((' '.join(current_aspect), current_label))\n",
    "                        current_aspect = []\n",
    "                        current_label = label_str[2:]\n",
    "                    elif label_str.startswith('I-'):\n",
    "                        current_label = label_str[2:]\n",
    "            \n",
    "            if current_aspect:\n",
    "                true_labels.append((' '.join(current_aspect), current_label))\n",
    "                \n",
    "            # Compare predictions with true labels\n",
    "            if pred_aspects != true_labels:\n",
    "                errors.append({\n",
    "                    'text': text,\n",
    "                    'predicted': pred_aspects,\n",
    "                    'true': true_labels\n",
    "                })\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Unknown label ID encountered: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db3ac6-4838-4e27-8f66-ca0bf0d7884a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ca48e-b535-4d8b-8eee-c909c3e6bb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c993faa-a113-4254-8bca-b4496f6e402b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4e44d7-f139-46d1-8985-025ad1d76127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
